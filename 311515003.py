# -*- coding: utf-8 -*-
"""model_compression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lSpFYr_rPeXW2I3_d4A3yfk-tppZ8eRx
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18
import torchvision.models as models
from torchsummary import summary
import pandas as pd
from torchvision.models import resnet50
import torch.nn.functional as F
import torch.utils.data as data

class ResNet(nn.Module):
  def __init__(self):
    super(ResNet, self).__init__()
    self.resnet50 = resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
    num_ftrs = self.resnet50.fc.in_features
    self.resnet50.fc = nn.Linear(num_ftrs, 10)

  def forward(self, x):
    x = self.resnet50.conv1(x)
    x = self.resnet50.bn1(x)
    x = self.resnet50.relu(x)
    x = self.resnet50.maxpool(x)

    x = self.resnet50.layer1(x)
    x = self.resnet50.layer2(x)
    x = self.resnet50.layer3(x)
    x = self.resnet50.layer4(x)

    x = self.resnet50.avgpool(x)
    x = torch.flatten(x, 1)
    x = self.resnet50.fc(x)
    return x

class student(nn.Module):
  def __init__(self,width_mult=1):
    super(student, self).__init__()
    bandwidth = [16, 16, 16, 32,32 , 32, 64, 64,64,64,128,128,128,220]

    self.cnn = nn.Sequential(
        nn.Sequential(
            nn.Conv2d(3, bandwidth[0], 3, 1, 1),
            nn.BatchNorm2d(bandwidth[0]),
            nn.ReLU(),
        ),
       
        nn.Sequential(
            nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),
            nn.BatchNorm2d(bandwidth[0]),
            nn.ReLU(),
            nn.Conv2d(bandwidth[0], bandwidth[1], 1),
            nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),
            nn.BatchNorm2d(bandwidth[1]),
            nn.ReLU(),
            nn.Conv2d(bandwidth[1], bandwidth[2], 1),
            nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),
            nn.BatchNorm2d(bandwidth[2]),
            nn.ReLU(),
            nn.Conv2d(bandwidth[2], bandwidth[3], 1),
            nn.MaxPool2d(2, 2, 0),
            nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),
            nn.BatchNorm2d(bandwidth[3]),
            nn.ReLU(),
            nn.Conv2d(bandwidth[3], bandwidth[4], 1),
            nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),
            nn.BatchNorm2d(bandwidth[4]),
            nn.ReLU(),
            nn.Conv2d(bandwidth[4], bandwidth[5], 1),
            nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),
            nn.BatchNorm2d(bandwidth[5]),
            nn.ReLU(),
            nn.Conv2d(bandwidth[5], bandwidth[6], 1),
            nn.MaxPool2d(2, 2, 0),
            nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),
            nn.BatchNorm2d(bandwidth[6]),
            nn.ReLU(),
            nn.Conv2d(bandwidth[6], bandwidth[7], 1),
            nn.Conv2d(bandwidth[7], bandwidth[7], 3, 1, 1, groups=bandwidth[7]),
            nn.BatchNorm2d(bandwidth[7]),
            nn.ReLU6(),
            nn.Conv2d(bandwidth[7], bandwidth[8], 1),
            nn.Conv2d(bandwidth[8], bandwidth[8], 3, 1, 1, groups=bandwidth[8]),
            nn.BatchNorm2d(bandwidth[8]),
            nn.ReLU6(),
            nn.Conv2d(bandwidth[8], bandwidth[9], 1),
            nn.Conv2d(bandwidth[9], bandwidth[9], 3, 1, 1, groups=bandwidth[9]),
            nn.BatchNorm2d(bandwidth[9]),
            nn.ReLU6(),
            nn.Conv2d(bandwidth[9], bandwidth[10], 1),
            nn.MaxPool2d(2, 2, 0),
            nn.Conv2d(bandwidth[10], bandwidth[10], 3, 1, 1, groups=bandwidth[10]),
            nn.BatchNorm2d(bandwidth[10]),
            nn.ReLU6(),
            nn.Conv2d(bandwidth[10], bandwidth[11], 1),
            nn.Conv2d(bandwidth[11], bandwidth[11], 3, 1, 1, groups=bandwidth[11]),
            nn.BatchNorm2d(bandwidth[11]),
            nn.ReLU6(),
            nn.Conv2d(bandwidth[11], bandwidth[12], 1),
            nn.Conv2d(bandwidth[12], bandwidth[12], 3, 1, 1, groups=bandwidth[12]),
            nn.BatchNorm2d(bandwidth[12]),
            nn.ReLU(),
            nn.Conv2d(bandwidth[12], bandwidth[13], 1),
            nn.MaxPool2d(2, 2, 0),
        ),
        nn.AdaptiveAvgPool2d((1, 1)),
    )
    self.fc = nn.Sequential(
        nn.Linear(bandwidth[13], 10),
    )

  def forward(self, x):
    out = self.cnn(x)
    out = out.view(out.size()[0], -1)
    return self.fc(out)

def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):
    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)
    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),
                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)
    return hard_loss + soft_loss

def dataloader():
  transform = transforms.Compose(
      [transforms.Grayscale(num_output_channels=3),
      transforms.ToTensor(),
      transforms.Normalize((0.5,), (0.5,))])
  transform_tr = transforms.Compose(
      [transforms.Grayscale(num_output_channels=3),
      transforms.RandomHorizontalFlip(),
      transforms.ToTensor(),
      transforms.Normalize((0.5,), (0.5,))])

  testset = torchvision.datasets.FashionMNIST(root='./data', train=False,
                                            download=True, transform=transform)
  testloader = torch.utils.data.DataLoader(testset, batch_size=1,
                                            shuffle=False, num_workers=0)
  trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,
                                            download=True, transform=transformï¼¿tr)
  train_set_size = int(len(trainset) * 0.8)
  valid_set_size = len(trainset) - train_set_size
  train_set, valid_set = data.random_split(trainset, [train_set_size, valid_set_size])
  trainloader = torch.utils.data.DataLoader(train_set, batch_size=128,
                                          shuffle=True, num_workers=0)
  validloader = torch.utils.data.DataLoader(valid_set, batch_size=128,
                                          shuffle=False, num_workers=0)

def training(alpha=0.5):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Testing on {device}")

    teacher_net = ResNet().to(device)

    checkpoint = torch.load("resnet-50.pth")
    teacher_net.load_state_dict(checkpoint['model_state_dict'])
    student_net = student().to(device)
    optimizer = optim.Adam(student_net.parameters(), lr=1e-3)
    
    teacher_net.eval()
    now_best_acc = 0
    for epoch in range(50):
      student_net.train()

      total_num, total_hit, total_loss = 0, 0, 0
      for i, batch_data in enumerate(trainloader):
          optimizer.zero_grad()
          inputs, hard_labels = batch_data
          inputs = inputs.cuda()
          hard_labels = torch.LongTensor(hard_labels).cuda()
          with torch.no_grad():
              soft_labels = teacher_net(inputs)

          logits = student_net(inputs)
          loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)
          loss.backward()
          optimizer.step()
            
          total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()
          total_num += len(inputs)

          total_loss += loss.item() * len(inputs)
          train_loss = total_loss / total_num 
          train_acc = total_hit / total_num
          
      student_net.eval()
      total_num, total_hit, total_loss = 0, 0, 0
      for i, batch_data in enumerate(validloader):
          optimizer.zero_grad()
          inputs, hard_labels = batch_data
          inputs = inputs.cuda()
          hard_labels = torch.LongTensor(hard_labels).cuda()
          with torch.no_grad():
              soft_labels = teacher_net(inputs)
              logits = student_net(inputs)
              loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)
            
          total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()
          total_num += len(inputs)

          total_loss += loss.item() * len(inputs)
          valid_loss = total_loss / total_num 
          valid_acc = total_hit / total_num   
      if valid_acc > now_best_acc:
          now_best_acc = valid_acc
          torch.save(student_net.state_dict(), '311515003.bin') 
      print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(
        epoch+1, train_loss, train_acc, valid_loss, valid_acc))

def result():
  net = student().cuda()
  net.load_state_dict(torch.load('311515003.bin'))
  net.eval()

    
  summary(net, (3, 28, 28))

  correct = 0
  total = 0
  pred_arr = []
  with torch.no_grad():
      for data in testloader:
          images, labels = data
          images, labels = images.to(device), labels.to(device)
          outputs = net(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
          pred_arr.append(predicted.item())

  accuracy = 100 * correct / total
  print(f"Accuracy of the network on the {total} test images: {accuracy:.2f} %")

  pred_data = {"pred":pred_arr}
  df_pred = pd.DataFrame(pred_data)
  df_pred.to_csv('example_pred.csv', index_label='id')

dataloader()
training()

result()
